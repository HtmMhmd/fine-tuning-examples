{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HtmMhmd/fine-tuning-examples/blob/main/gemma3_grpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b971a5",
      "metadata": {
        "id": "12b971a5"
      },
      "source": [
        "# GRPO Fine-tuning with Gemma 3 (1B): Compact Model Optimization\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **GRPO (Gradient Ratio Policy Optimization)** fine-tuning using the compact **Gemma 3 1B** model. Unlike the larger DeepSeek R1 8B model, Gemma 3 1B presents unique challenges and opportunities:\n",
        "\n",
        "### Why Gemma 3 1B?\n",
        "- **Resource Efficiency**: Fits on consumer GPUs with limited VRAM\n",
        "- **Fast Iteration**: Quick training cycles for experimentation\n",
        "- **Edge Deployment**: Suitable for mobile and edge computing scenarios\n",
        "- **Educational Value**: Easier to understand and debug due to smaller scale\n",
        "\n",
        "### Key Learning Objectives\n",
        "1. **Compact Model Optimization**: How to maximize performance from small models\n",
        "2. **GRPO Scaling**: Understanding how GRPO behaves with different model sizes\n",
        "3. **Reward Function Adaptation**: Adjusting reward systems for smaller capacity models\n",
        "4. **Resource Constraint Management**: Working within memory and compute limitations\n",
        "\n",
        "### Technical Challenges\n",
        "- **Limited Capacity**: 1B parameters vs 8B in DeepSeek R1\n",
        "- **Knowledge Constraints**: Smaller models have less world knowledge\n",
        "- **Generalization**: Ensuring the model doesn't overfit to specific patterns\n",
        "- **Language Consistency**: Maintaining Indonesian language usage with limited capacity\n",
        "\n",
        "### Expected Outcomes\n",
        "By the end of this notebook, you'll understand how GRPO can effectively fine-tune even compact models for specific tasks while maintaining efficiency and performance quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f5af05",
      "metadata": {
        "id": "45f5af05"
      },
      "source": [
        "## 1. Environment Setup and Dependencies\n",
        "\n",
        "### Memory-Optimized Configuration\n",
        "Since we're working with a 1B model, we can use more aggressive optimization techniques that might not be suitable for larger models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "TtbbABUppQay",
      "metadata": {
        "collapsed": true,
        "id": "TtbbABUppQay"
      },
      "outputs": [],
      "source": [
        "!pip install -q unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ukYXU_BtqXmL",
      "metadata": {
        "id": "ukYXU_BtqXmL"
      },
      "outputs": [],
      "source": [
        "!pip install -q langid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8b46e3d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b46e3d8",
        "outputId": "7fd7d6b6-8e77-4f42-c231-1ac57f9fe2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-143939625.py:11: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "üöÄ Gemma 3 1B GRPO Training Environment Ready!\n",
            "PyTorch version: 2.7.1+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Memory-optimized imports for compact model training\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Single GPU is sufficient\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"  # Smaller chunks for 1B model\n",
        "\n",
        "# Core libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "import langid\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ Gemma 3 1B GRPO Training Environment Ready!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "OmLoDq2bwAkp",
      "metadata": {
        "id": "OmLoDq2bwAkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ab6909-3dd6-4abb-d8e9-84c18ed499b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Language detection configured for Indonesian enforcement\n",
            "\n",
            "üîç Testing improved language detection:\n",
            "Test 1: Untuk menyelesaikan persamaan 2x + 3 = 7, kita per...\n",
            "  Detected: id (raw: id, conf: -105.298)\n",
            "\n",
            "Test 2: To solve 2x + 3 = 7, subtract 3 from both sides....\n",
            "  Detected: en (raw: en, conf: -24.775)\n",
            "\n",
            "Test 3: Jawaban: x = 2. Langkah pertama kurangi 3, langkah...\n",
            "  Detected: id (raw: id, conf: -101.698)\n",
            "\n",
            "Test 4: The answer is 5...\n",
            "  Detected: en (raw: en, conf: -49.995)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configure langid for Indonesian detection\n",
        "langid.set_languages(['id', 'en'])  # Focus on Indonesian and English\n",
        "\n",
        "def get_lang(text: str) -> str:\n",
        "    \"\"\"Detect language with improved confidence handling\"\"\"\n",
        "    # Clean text for better detection\n",
        "    cleaned_text = text.strip().lower()\n",
        "\n",
        "    # Try langid classification\n",
        "    lang, confidence = langid.classify(cleaned_text)\n",
        "\n",
        "    # Improved confidence threshold and fallback logic\n",
        "    if confidence > 0.5:  # Lower threshold for better detection\n",
        "        return lang\n",
        "\n",
        "    # Fallback: Check for Indonesian keywords\n",
        "    indonesian_keywords = [\n",
        "        'adalah', 'untuk', 'dengan', 'dari', 'yang', 'dan', 'dalam', 'pada',\n",
        "        'jawaban', 'hasil', 'solusi', 'persamaan', 'menyelesaikan', 'langkah',\n",
        "        'pertama', 'kedua', 'terakhir', 'sama dengan', 'kita', 'perlu'\n",
        "    ]\n",
        "\n",
        "    # Check if text contains Indonesian keywords\n",
        "    word_count = len(cleaned_text.split())\n",
        "    indonesian_word_count = sum(1 for word in indonesian_keywords if word in cleaned_text)\n",
        "\n",
        "    if word_count > 0 and (indonesian_word_count / word_count) > 0.2:\n",
        "        return 'id'\n",
        "\n",
        "    # Check for English keywords as secondary fallback\n",
        "    english_keywords = ['the', 'and', 'to', 'of', 'a', 'is', 'for', 'solve', 'equation', 'answer']\n",
        "    english_word_count = sum(1 for word in english_keywords if word in cleaned_text)\n",
        "\n",
        "    if word_count > 0 and (english_word_count / word_count) > 0.2:\n",
        "        return 'en'\n",
        "\n",
        "    return 'unknown'\n",
        "\n",
        "# Test the improved language detection\n",
        "print(\"‚úÖ Language detection configured for Indonesian enforcement\")\n",
        "print(\"\\nüîç Testing improved language detection:\")\n",
        "\n",
        "test_sentences = [\n",
        "    \"Untuk menyelesaikan persamaan 2x + 3 = 7, kita perlu mengurangi 3 dari kedua sisi.\",\n",
        "    \"To solve 2x + 3 = 7, subtract 3 from both sides.\",\n",
        "    \"Jawaban: x = 2. Langkah pertama kurangi 3, langkah kedua bagi dengan 2.\",\n",
        "    \"The answer is 5\"\n",
        "]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    detected_lang = get_lang(sentence)\n",
        "    raw_lang, raw_conf = langid.classify(sentence)\n",
        "    print(f\"Test {i}: {sentence[:50]}...\")\n",
        "    print(f\"  Detected: {detected_lang} (raw: {raw_lang}, conf: {raw_conf:.3f})\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a64bd9",
      "metadata": {
        "id": "84a64bd9"
      },
      "source": [
        "## 2. Gemma 3 1B Model Loading and Configuration\n",
        "\n",
        "### Compact Model Optimization Strategy\n",
        "For the 1B model, we can use more aggressive LoRA configurations and higher learning rates since the model is less prone to catastrophic forgetting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "41d01886",
      "metadata": {
        "id": "41d01886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76926472-8491-4551-9d7c-7d9c572dd898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading Gemma 3 1B model with Unsloth optimizations...\n",
            "==((====))==  Unsloth 2025.7.8: Fast Gemma3 patching. Transformers: 4.53.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
            "‚úÖ Model loaded: unsloth/gemma-3-1b-it\n",
            "üìä Model parameters: ~1B\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Will map <end_of_turn> to EOS = <end_of_turn>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß LoRA Configuration for Compact Model:\n",
            "  ‚Ä¢ Rank (r): 64 (higher than typical for compensation)\n",
            "  ‚Ä¢ Alpha: 128 (strong adaptation signal)\n",
            "  ‚Ä¢ Target modules: 9 layers\n",
            "  ‚Ä¢ Dropout: 0.1 (balanced regularization)\n",
            "‚úÖ Gemma 3 1B model ready for GRPO training!\n",
            "üìà Total parameters: 1,025,977,472\n",
            "üéØ Trainable parameters: 26,091,520 (2.54%)\n"
          ]
        }
      ],
      "source": [
        "# Gemma 3 1B model configuration with aggressive optimization\n",
        "model_name = \"unsloth/gemma-3-1b-it\"  # Gemma 3 1B instruction-tuned\n",
        "max_seq_length = 2048  # Sufficient for mathematical problems\n",
        "\n",
        "print(\"üîÑ Loading Gemma 3 1B model with Unsloth optimizations...\")\n",
        "\n",
        "# Load model with higher LoRA rank for compact models\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    # dtype=torch.float16,  # Use FP16 for memory efficiency\n",
        "    # load_in_4bit=True,   # 4-bit quantization for even smaller memory footprint\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {model_name}\")\n",
        "print(f\"üìä Model parameters: ~1B\")\n",
        "# print(f\"üíæ Memory optimization: 4-bit quantization + FP16\")\n",
        "\n",
        "# Configure LoRA with higher rank for compact models\n",
        "# Higher rank compensates for smaller base model capacity\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,               # Higher rank than typical (usually 16-32)\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        # \"embed_tokens\", \"lm_head\"  # Include embeddings for better adaptation\n",
        "    ],\n",
        "    lora_alpha=64,     # Higher alpha for stronger adaptation\n",
        "    lora_dropout=0.1,   # Moderate dropout to prevent overfitting\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"üîß LoRA Configuration for Compact Model:\")\n",
        "print(f\"  ‚Ä¢ Rank (r): 64 (higher than typical for compensation)\")\n",
        "print(f\"  ‚Ä¢ Alpha: 128 (strong adaptation signal)\")\n",
        "print(f\"  ‚Ä¢ Target modules: {len(['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'embed_tokens', 'lm_head'])} layers\")\n",
        "print(f\"  ‚Ä¢ Dropout: 0.1 (balanced regularization)\")\n",
        "\n",
        "# Configure chat template for Gemma\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"gemma\",  # Use Gemma-specific template\n",
        "    map_eos_token=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Gemma 3 1B model ready for GRPO training!\")\n",
        "\n",
        "# Model statistics\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"üìà Total parameters: {total_params:,}\")\n",
        "print(f\"üéØ Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d737976",
      "metadata": {
        "id": "6d737976"
      },
      "source": [
        "## 3. Compact Model Reward System Design\n",
        "\n",
        "### Adaptation for Limited Capacity\n",
        "\n",
        "With only 1B parameters, we need to design a more focused reward system that doesn't overwhelm the model's limited capacity. The key is **reward simplification** while maintaining effectiveness.\n",
        "\n",
        "### Design Principles for Small Models\n",
        "1. **Fewer Competing Objectives**: Reduce cognitive load on the model\n",
        "2. **Clearer Reward Signals**: More distinct positive/negative feedback\n",
        "3. **Progressive Complexity**: Start simple, gradually increase sophistication\n",
        "4. **Emphasis on Critical Features**: Focus on the most important behaviors first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4a2b4fdc",
      "metadata": {
        "id": "4a2b4fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0286d0c-a099-4b45-ed72-61d1f1e00c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Designing compact model reward system...\n",
            "‚úÖ Compact Model Reward System Initialized\n",
            "üìä Reward Weights: {'language': 0.6, 'correctness': 0.3, 'clarity': 0.1}\n",
            "üéØ Focus: Indonesian language usage (60% weight)\n",
            "üßÆ Secondary: Mathematical correctness (30% weight)\n",
            "üìù Tertiary: Response clarity (10% weight)\n",
            "\n",
            "üß™ Testing Reward System:\n",
            "\n",
            "Test 1: Untuk menyelesaikan persamaan 2x + 3 = 7, kita per...\n",
            "  Total Reward: 0.800\n",
            "  Language: 1.000 (id)\n",
            "  Correctness: 0.500\n",
            "  Clarity: 0.500\n",
            "\n",
            "Test 2: To solve 2x + 3 = 7, subtract 3 from both sides....\n",
            "  Total Reward: -0.190\n",
            "  Language: -0.500 (en)\n",
            "  Correctness: 0.200\n",
            "  Clarity: 0.500\n",
            "\n",
            "Test 3: Jawaban: x = 2. Langkah pertama kurangi 3, langkah...\n",
            "  Total Reward: 0.860\n",
            "  Language: 1.000 (id)\n",
            "  Correctness: 0.800\n",
            "  Clarity: 0.200\n"
          ]
        }
      ],
      "source": [
        "# Simplified reward system optimized for 1B parameter model\n",
        "print(\"üéØ Designing compact model reward system...\")\n",
        "\n",
        "# Indonesian system prompt - simplified for better adherence\n",
        "system_prompt = \"\"\"Anda adalah asisten matematika yang menjawab dalam bahasa Indonesia.\n",
        "Berikan penjelasan yang jelas dan mudah dipahami.\"\"\"\n",
        "\n",
        "class CompactModelRewardSystem:\n",
        "    \"\"\"Simplified reward system designed for small language models\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.weights = {\n",
        "            'language': 0.6,      # Primary focus: Indonesian usage\n",
        "            'correctness': 0.3,   # Secondary: mathematical accuracy\n",
        "            'clarity': 0.1        # Tertiary: response clarity\n",
        "        }\n",
        "\n",
        "    def calculate_language_reward(self, response: str) -> float:\n",
        "        \"\"\"Primary reward: Indonesian language usage\"\"\"\n",
        "        lang = get_lang(response)\n",
        "\n",
        "        if lang == 'id':\n",
        "            return 1.0  # Perfect score for Indonesian\n",
        "        elif lang == 'en':\n",
        "            return -0.5  # Penalty for English\n",
        "        else:\n",
        "            return -0.8  # Larger penalty for other languages\n",
        "\n",
        "    def calculate_correctness_reward(self, response: str, question: str) -> float:\n",
        "        \"\"\"Simplified correctness check for mathematical content\"\"\"\n",
        "        response_lower = response.lower()\n",
        "\n",
        "        # Basic mathematical indicators\n",
        "        math_indicators = [\n",
        "            'jawaban', 'hasil', 'solusi', '=', 'sama dengan',\n",
        "            'langkah', 'pertama', 'kedua', 'terakhir'\n",
        "        ]\n",
        "\n",
        "        # Count mathematical reasoning indicators\n",
        "        indicator_count = sum(1 for indicator in math_indicators\n",
        "                            if indicator in response_lower)\n",
        "\n",
        "        # Simple scoring based on presence of mathematical language\n",
        "        if indicator_count >= 3:\n",
        "            return 0.8  # Good mathematical reasoning\n",
        "        elif indicator_count >= 2:\n",
        "            return 0.5  # Moderate reasoning\n",
        "        elif indicator_count >= 1:\n",
        "            return 0.2  # Minimal reasoning\n",
        "        else:\n",
        "            return -0.3  # No mathematical reasoning detected\n",
        "\n",
        "    def calculate_clarity_reward(self, response: str) -> float:\n",
        "        \"\"\"Simplified clarity assessment\"\"\"\n",
        "        # Basic clarity indicators\n",
        "        words = response.split()\n",
        "        sentences = response.split('.')\n",
        "\n",
        "        # Optimal length range for 1B model responses\n",
        "        if 20 <= len(words) <= 100:\n",
        "            length_score = 0.5\n",
        "        elif 10 <= len(words) <= 150:\n",
        "            length_score = 0.2\n",
        "        else:\n",
        "            length_score = -0.2\n",
        "\n",
        "        # Sentence structure bonus\n",
        "        avg_sentence_length = len(words) / max(len(sentences), 1)\n",
        "        if 5 <= avg_sentence_length <= 20:\n",
        "            structure_score = 0.3\n",
        "        else:\n",
        "            structure_score = 0.0\n",
        "\n",
        "        return length_score + structure_score\n",
        "\n",
        "    def compute_reward(self, query: str, response: str, **kwargs) -> Dict[str, float]:\n",
        "        \"\"\"Compute final reward with detailed breakdown\"\"\"\n",
        "        # Calculate individual rewards\n",
        "        lang_reward = self.calculate_language_reward(response)\n",
        "        correct_reward = self.calculate_correctness_reward(response, query)\n",
        "        clarity_reward = self.calculate_clarity_reward(response)\n",
        "\n",
        "        # Weighted combination\n",
        "        total_reward = (\n",
        "            self.weights['language'] * lang_reward +\n",
        "            self.weights['correctness'] * correct_reward +\n",
        "            self.weights['clarity'] * clarity_reward\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'total': float(total_reward),\n",
        "            'language': float(lang_reward),\n",
        "            'correctness': float(correct_reward),\n",
        "            'clarity': float(clarity_reward),\n",
        "            'breakdown': {\n",
        "                'detected_language': get_lang(response),\n",
        "                'response_length': len(response.split()),\n",
        "                'weights_used': self.weights\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Initialize the compact reward system\n",
        "reward_system = CompactModelRewardSystem()\n",
        "\n",
        "print(\"‚úÖ Compact Model Reward System Initialized\")\n",
        "print(f\"üìä Reward Weights: {reward_system.weights}\")\n",
        "print(\"üéØ Focus: Indonesian language usage (60% weight)\")\n",
        "print(\"üßÆ Secondary: Mathematical correctness (30% weight)\")\n",
        "print(\"üìù Tertiary: Response clarity (10% weight)\")\n",
        "\n",
        "# Test the reward system\n",
        "test_responses = [\n",
        "    \"Untuk menyelesaikan persamaan 2x + 3 = 7, kita perlu mengurangi 3 dari kedua sisi.\",\n",
        "    \"To solve 2x + 3 = 7, subtract 3 from both sides.\",\n",
        "    \"Jawaban: x = 2. Langkah pertama kurangi 3, langkah kedua bagi dengan 2.\"\n",
        "]\n",
        "\n",
        "print(\"\\nüß™ Testing Reward System:\")\n",
        "for i, response in enumerate(test_responses, 1):\n",
        "    reward = reward_system.compute_reward(\"Solve 2x + 3 = 7\", response)\n",
        "    print(f\"\\nTest {i}: {response[:50]}...\")\n",
        "    print(f\"  Total Reward: {reward['total']:.3f}\")\n",
        "    print(f\"  Language: {reward['language']:.3f} ({reward['breakdown']['detected_language']})\")\n",
        "    print(f\"  Correctness: {reward['correctness']:.3f}\")\n",
        "    print(f\"  Clarity: {reward['clarity']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "id": "5W6_KMn16gYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdf8d68-5f62-47b7-9a15-c56ce2c4ed48"
      },
      "id": "5W6_KMn16gYm",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total': 0.86,\n",
              " 'language': 1.0,\n",
              " 'correctness': 0.8,\n",
              " 'clarity': 0.2,\n",
              " 'breakdown': {'detected_language': 'id',\n",
              "  'response_length': 13,\n",
              "  'weights_used': {'language': 0.6, 'correctness': 0.3, 'clarity': 0.1}}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad08ef81",
      "metadata": {
        "id": "ad08ef81"
      },
      "source": [
        "## 4. Dataset Preparation for Compact Models\n",
        "\n",
        "### Curated Learning Strategy\n",
        "For 1B models, we need **high-quality, focused datasets** rather than large volumes of data. The model's limited capacity requires careful curation to avoid confusion and ensure effective learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f45b19ef",
      "metadata": {
        "id": "f45b19ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ff3365-f465-4fc6-f26a-ab0c8c5c05a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Preparing focused dataset for Gemma 3 1B...\n",
            "üìä Dataset size: 8 examples\n",
            "üéØ Focus areas: Basic algebra, geometry, arithmetic\n",
            "üáÆüá© Language target: Indonesian responses\n",
            "‚úÖ 8 prompts formatted for GRPO training\n",
            "\n",
            "üìù Sample formatted prompt:\n",
            "<bos><start_of_turn>user\n",
            "Anda adalah asisten matematika yang menjawab dalam bahasa Indonesia.\n",
            "Berikan penjelasan yang jelas dan mudah dipahami. Solve the equation 2x + 5 = 15<end_of_turn>\n",
            "<start_of_tu...\n",
            "\n",
            "‚öôÔ∏è GRPO Configuration for Compact Model:\n",
            "üìà Learning rate: 0.0003\n",
            "üîÑ Epochs: 5\n",
            "üì¶ Batch size: 4 √ó 8 = 32\n",
            "üéõÔ∏è Beta (KL penalty): 0.01\n",
            "üíæ FP16 enabled: False\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Focused dataset for 1B model training\n",
        "from datasets import Dataset\n",
        "print(\"üìö Preparing focused dataset for Gemma 3 1B...\")\n",
        "\n",
        "# Smaller, more focused dataset for compact models\n",
        "training_data = [\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": \"Solve the equation 2x + 5 = 15\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"The solution to 2x + 5 = 15 is x = 5.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": \"Find the area of a circle with radius 3\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"The area of a circle with radius 3 is approximately 28.27 square units.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": \"What is 15% of 80?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"15% of 80 is 12.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": \"Solve x^2 - 4x + 4 = 0\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"The solution to x^2 - 4x + 4 = 0 is x = 2.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": \"Convert 2.5 hours to minutes\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"2.5 hours is equal to 150 minutes.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": \"Find the perimeter of a rectangle with length 8 and width 5\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"The perimeter of a rectangle with length 8 and width 5 is 26 units.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": \"What is the square root of 64?\"},\n",
        "             {\"role\": \"assistant\", \"content\": \"The square root of 64 is 8.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": \"Simplify the fraction 15/25\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"The simplified fraction 15/25 is 3/5.\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"üìä Dataset size: {len(training_data)} examples\")\n",
        "print(\"üéØ Focus areas: Basic algebra, geometry, arithmetic\")\n",
        "print(\"üáÆüá© Language target: Indonesian responses\")\n",
        "\n",
        "# Convert to text format for GRPO\n",
        "def format_training_data(data):\n",
        "    \"\"\"Format data for GRPO training\"\"\"\n",
        "    formatted_data = []\n",
        "\n",
        "    for item in data:\n",
        "        # Apply chat template\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            item[\"messages\"],\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=False,\n",
        "        )\n",
        "        formatted_data.append({\"prompt\": text})\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "formatted_prompts = format_training_data(training_data)\n",
        "train_dataset = Dataset.from_list(formatted_prompts)\n",
        "\n",
        "print(f\"‚úÖ {len(train_dataset)} prompts formatted for GRPO training\")\n",
        "print(\"\\nüìù Sample formatted prompt:\")\n",
        "print(train_dataset[0]['prompt'][:200] + \"...\")\n",
        "\n",
        "# Compact model training configuration\n",
        "print(\"\\n‚öôÔ∏è GRPO Configuration for Compact Model:\")\n",
        "\n",
        "max_prompt_length = 256\n",
        "\n",
        "grpo_config = GRPOConfig(\n",
        "    # Learning parameters optimized for 1B model\n",
        "    learning_rate=3e-4,           # Higher LR for faster adaptation\n",
        "    num_train_epochs=5,           # More epochs for smaller dataset\n",
        "    per_device_train_batch_size=4, # Small batch size for memory efficiency. Match this to dataset number for testing.\n",
        "    gradient_accumulation_steps=8, # Effective batch size = 8\n",
        "\n",
        "    # GRPO specific parameters\n",
        "    beta=0.01,                    # Lower beta for gentler policy updates\n",
        "    num_generations=4,            # Explicitly set number of generations\n",
        "\n",
        "    # Optimization for compact models\n",
        "    warmup_steps=10,             # Quick warmup\n",
        "    logging_steps=1,             # Frequent logging for monitoring\n",
        "    save_steps=50,               # Regular checkpoints\n",
        "    eval_steps=25,               # Frequent evaluation\n",
        "\n",
        "    # Memory optimization\n",
        "    dataloader_drop_last=True, #Very important for you\n",
        "    remove_unused_columns=True, #Change back to True\n",
        "\n",
        "    # Output configuration\n",
        "    output_dir=\"./grpo_gemma_1b_results\",\n",
        "    logging_dir=\"./grpo_gemma_1b_logs\",\n",
        "\n",
        "    # Regularization for small models\n",
        "    weight_decay=0.01,           # Light weight decay\n",
        "    max_grad_norm=1.0,           # Gradient clipping\n",
        "\n",
        "    # Advanced settings\n",
        "    # fp16=True,                   # Use FP16 for memory efficiency\n",
        "    # report_to=\"none\",              # Disable wandb for simplicity\n",
        "\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_torch_fused\",\n",
        "\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_seq_length - max_prompt_length,\n",
        "\n",
        "    )\n",
        "\n",
        "print(f\"üìà Learning rate: {grpo_config.learning_rate}\")\n",
        "print(f\"üîÑ Epochs: {grpo_config.num_train_epochs}\")\n",
        "print(f\"üì¶ Batch size: {grpo_config.per_device_train_batch_size} √ó {grpo_config.gradient_accumulation_steps} = {grpo_config.per_device_train_batch_size * grpo_config.gradient_accumulation_steps}\")\n",
        "print(f\"üéõÔ∏è Beta (KL penalty): {grpo_config.beta}\")\n",
        "print(f\"üíæ FP16 enabled: {grpo_config.fp16}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb28524",
      "metadata": {
        "id": "2eb28524"
      },
      "source": [
        "## 5. GRPO Training Execution for Compact Models\n",
        "\n",
        "### Memory-Efficient Training Strategy\n",
        "With Gemma 3 1B, we can afford more aggressive training schedules and frequent evaluations due to the reduced computational overhead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ca1c80df",
      "metadata": {
        "id": "ca1c80df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef731d9-9e1d-49e5-c7fe-56694e6d0c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting GRPO training for Gemma 3 1B...\n",
            "‚è±Ô∏è Expected training time: ~10-15 minutes (much faster than 8B models)\n",
            "Unsloth: Switching to float32 training since model cannot work with float16\n",
            "‚úÖ GRPO Trainer initialized\n",
            "üìä Training on 8 examples\n",
            "üîÑ 5 epochs √ó 8 examples = 40 total steps\n"
          ]
        }
      ],
      "source": [
        "# Execute GRPO training for Gemma 3 1B\n",
        "print(\"üöÄ Starting GRPO training for Gemma 3 1B...\")\n",
        "print(\"‚è±Ô∏è Expected training time: ~10-15 minutes (much faster than 8B models)\")\n",
        "\n",
        "def reward_adapter(prompts, completions, completion_ids, **kwargs):\n",
        "    rewards = []\n",
        "    for prompt, completion in zip(prompts, completions):\n",
        "        reward_output = reward_system.compute_reward(prompt, completion)\n",
        "        rewards.append(reward_output['total'])\n",
        "    return torch.tensor(rewards, dtype=torch.float32)\n",
        "\n",
        "# Initialize GRPO trainer with compact model optimizations\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=grpo_config,\n",
        "    reward_funcs=reward_adapter,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GRPO Trainer initialized\")\n",
        "print(f\"üìä Training on {len(train_dataset)} examples\")\n",
        "print(f\"üîÑ {grpo_config.num_train_epochs} epochs √ó {len(train_dataset)} examples = {grpo_config.num_train_epochs * len(train_dataset)} total steps\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print(\"\\nüéØ Beginning GRPO training...\")\n",
        "import torch\n",
        "torch._dynamo.config.disable = True\n",
        "training_result = trainer.train()\n",
        "torch._dynamo.config.disable = False\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Training completed!\")\n",
        "# print(f\"üìà Final loss: {training_result.training_loss:.4f}\")\n",
        "# print(f\"‚è±Ô∏è Training time: {training_result.training_time:.2f} seconds\")\n",
        "\n",
        "# Save the trained model\n",
        "print(\"\\nüíæ Saving trained model...\")\n",
        "model.save_pretrained(\"gemma_1b_grpo_lora\")\n",
        "print(\"‚úÖ Model saved as 'gemma_1b_grpo_lora'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "i82uYzAYBgfb",
        "outputId": "94ddd507-3784-4072-886a-1785211999c5"
      },
      "id": "i82uYzAYBgfb",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Beginning GRPO training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 8 | Num Epochs = 5 | Total steps = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 26,091,520 of 1,025,977,472 (2.54% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 09:38, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / reward_adapter / mean</th>\n",
              "      <th>rewards / reward_adapter / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>-0.243750</td>\n",
              "      <td>0.035307</td>\n",
              "      <td>346.656250</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>538.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>346.656250</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>538.000000</td>\n",
              "      <td>0.172516</td>\n",
              "      <td>-0.243750</td>\n",
              "      <td>0.054875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>-0.236563</td>\n",
              "      <td>0.024203</td>\n",
              "      <td>370.468750</td>\n",
              "      <td>212.000000</td>\n",
              "      <td>1007.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>370.468750</td>\n",
              "      <td>212.000000</td>\n",
              "      <td>1007.000000</td>\n",
              "      <td>0.169418</td>\n",
              "      <td>-0.236563</td>\n",
              "      <td>0.038656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>-0.236563</td>\n",
              "      <td>0.027435</td>\n",
              "      <td>377.875000</td>\n",
              "      <td>186.000000</td>\n",
              "      <td>548.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>377.875000</td>\n",
              "      <td>186.000000</td>\n",
              "      <td>548.000000</td>\n",
              "      <td>0.178324</td>\n",
              "      <td>-0.236563</td>\n",
              "      <td>0.045763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>-0.235625</td>\n",
              "      <td>0.022553</td>\n",
              "      <td>392.125000</td>\n",
              "      <td>204.000000</td>\n",
              "      <td>748.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>392.125000</td>\n",
              "      <td>204.000000</td>\n",
              "      <td>748.000000</td>\n",
              "      <td>0.191965</td>\n",
              "      <td>-0.235625</td>\n",
              "      <td>0.030894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>-0.235625</td>\n",
              "      <td>0.022457</td>\n",
              "      <td>375.875000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>752.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>375.875000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>752.000000</td>\n",
              "      <td>0.188307</td>\n",
              "      <td>-0.235625</td>\n",
              "      <td>0.027933</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training completed!\n",
            "\n",
            "üíæ Saving trained model...\n",
            "‚úÖ Model saved as 'gemma_1b_grpo_lora'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Quick evaluation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üß™ COMPACT MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_questions = [\n",
        "    \"What is 25% of 120?\",\n",
        "    \"Solve for x: 3x - 7 = 14\",\n",
        "    \"Find the circumference of a circle with radius 4\"\n",
        "]\n",
        "\n",
        "print(\"\\nüßÆ Testing Gemma 3 1B after GRPO training:\")\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    # Format the question\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "    )\n",
        "\n",
        "    # Generate response\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,  # Shorter responses for 1B model\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    # Evaluate the response\n",
        "    reward_breakdown = reward_system.compute_reward(question, response)\n",
        "\n",
        "    print(f\"\\nTest {i}: {question}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Language: {reward_breakdown['breakdown']['detected_language']}\")\n",
        "    print(f\"Total Reward: {reward_breakdown['total']:.3f}\")\n",
        "    print(f\"  ‚Ä¢ Language: {reward_breakdown['language']:.3f}\")\n",
        "    print(f\"  ‚Ä¢ Correctness: {reward_breakdown['correctness']:.3f}\")\n",
        "    print(f\"  ‚Ä¢ Clarity: {reward_breakdown['clarity']:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä COMPACT MODEL TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚úÖ Model: Gemma 3 1B (unsloth/gemma-2-2b-it)\")\n",
        "print(f\"üéØ Training Method: GRPO with simplified reward system\")\n",
        "# print(f\"üìä Dataset: {len(training_data)} focused mathematical examples\")\n",
        "# print(f\"‚è±Ô∏è Training Time: ~{training_result.training_time/60:.1f} minutes\")\n",
        "print(f\"üíæ Memory Usage: Optimized with 4-bit quantization + LoRA\")\n",
        "print(f\"üáÆüá© Language Target: Indonesian mathematical explanations\")\n",
        "print(f\"üîß LoRA Configuration: r=64, alpha=128 (higher for compensation)\")\n",
        "print(f\"üìà Key Insight: Compact models require focused datasets and higher LoRA ranks\")\n",
        "\n",
        "print(\"\\nüéì Educational Takeaways:\")\n",
        "print(\"  ‚Ä¢ 1B models can be effectively fine-tuned with GRPO\")\n",
        "print(\"  ‚Ä¢ Higher LoRA ranks compensate for smaller base capacity\")\n",
        "print(\"  ‚Ä¢ Simplified reward systems work better for compact models\")\n",
        "print(\"  ‚Ä¢ Focused datasets prevent confusion in small models\")\n",
        "print(\"  ‚Ä¢ Training is much faster, enabling rapid experimentation\")"
      ],
      "metadata": {
        "id": "cBYHP4_kBaVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1171bc4-9411-48b3-d23d-fdfd95d0e274"
      },
      "id": "cBYHP4_kBaVe",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üß™ COMPACT MODEL EVALUATION\n",
            "==================================================\n",
            "\n",
            "üßÆ Testing Gemma 3 1B after GRPO training:\n",
            "\n",
            "Test 1: What is 25% of 120?\n",
            "Response: Tentu, mari kita pecahkan soal ini:\n",
            "\n",
            "**1. Cara Menghitung:**\n",
            "\n",
            "Ada beberapa cara untuk menghitung 25% dari 120:\n",
            "\n",
            "*   **Cara 1: Bagi**\n",
            "    *   25% sama dengan 25/100, atau 0,25\n",
            "    *   0,25 * 120 = (1/4) * 120 = 30\n",
            "\n",
            "*   **Cara 2: Kalikan dengan 0,25**\n",
            "    *   120 dikalikan dengan 0,25: 120 * 0,25 = 30\n",
            "\n",
            "**Jadi, \n",
            "Language: unknown\n",
            "Total Reward: -0.280\n",
            "  ‚Ä¢ Language: -0.800\n",
            "  ‚Ä¢ Correctness: 0.500\n",
            "  ‚Ä¢ Clarity: 0.500\n",
            "\n",
            "Test 2: Solve for x: 3x - 7 = 14\n",
            "Response: Tentu, mari kita pecahkan persamaan ini:\n",
            "\n",
            "**1. Identifikasi Operasi yang Harus Dilakukan:**\n",
            "\n",
            "Kita memiliki persamaan: 3x - 7 = 14\n",
            "\n",
            "Kita perlu melakukan operasi yang akan mengubah persamaan menjadi bentuk yang bisa dipecahkan. Operasi yang perlu kita lakukan adalah pengurangan.\n",
            "\n",
            "**2. Selesaikan untuk x:**\n",
            "\n",
            "*   Kurangi 7 dari kedua sisi persamaan:\n",
            "   3x - 7 - 7 = 14 - 7\n",
            "   3x - 14 = 7\n",
            "\n",
            "*   Tambahkan 14 ke kedua sisi:\n",
            "   3x - 14 + 14 = 7 + 14\n",
            "   3x = \n",
            "Language: unknown\n",
            "Total Reward: -0.250\n",
            "  ‚Ä¢ Language: -0.800\n",
            "  ‚Ä¢ Correctness: 0.500\n",
            "  ‚Ä¢ Clarity: 0.800\n",
            "\n",
            "Test 3: Find the circumference of a circle with radius 4\n",
            "Response: Tentu, mari kita pecah cara mencari keliling lingkaran.\n",
            "\n",
            "**1. Apa itu Keliling?**\n",
            "\n",
            "Keliling lingkaran adalah jarak di sekeliling lingkaran.  Kita bisa menghitungnya dengan cara berikut:\n",
            "\n",
            "*   Keliling = 2 * œÄ * r\n",
            "\n",
            "Dimana:\n",
            "\n",
            "*   œÄ (pi) adalah konstanta matematika yang kira-kira sama dengan 3.14159\n",
            "*   r adalah jari-jari lingkaran (jarak dari pusat lingkaran ke tepi lingkaran)\n",
            "\n",
            "**2. Menghitung Keliling Lingkaran dengan Radius 4**\n",
            "\n",
            "*   **Jari-jari (r) = 4**\n",
            "*   Kita gunakan rumus:\n",
            "\n",
            "    Keliling = \n",
            "Language: unknown\n",
            "Total Reward: -0.250\n",
            "  ‚Ä¢ Language: -0.800\n",
            "  ‚Ä¢ Correctness: 0.500\n",
            "  ‚Ä¢ Clarity: 0.800\n",
            "\n",
            "==================================================\n",
            "üìä COMPACT MODEL TRAINING SUMMARY\n",
            "==================================================\n",
            "‚úÖ Model: Gemma 3 1B (unsloth/gemma-2-2b-it)\n",
            "üéØ Training Method: GRPO with simplified reward system\n",
            "üíæ Memory Usage: Optimized with 4-bit quantization + LoRA\n",
            "üáÆüá© Language Target: Indonesian mathematical explanations\n",
            "üîß LoRA Configuration: r=64, alpha=128 (higher for compensation)\n",
            "üìà Key Insight: Compact models require focused datasets and higher LoRA ranks\n",
            "\n",
            "üéì Educational Takeaways:\n",
            "  ‚Ä¢ 1B models can be effectively fine-tuned with GRPO\n",
            "  ‚Ä¢ Higher LoRA ranks compensate for smaller base capacity\n",
            "  ‚Ä¢ Simplified reward systems work better for compact models\n",
            "  ‚Ä¢ Focused datasets prevent confusion in small models\n",
            "  ‚Ä¢ Training is much faster, enabling rapid experimentation\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}